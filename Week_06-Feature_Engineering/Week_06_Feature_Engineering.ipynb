{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC4050/blob/main/Week_06-Feature_Engineering/Week_06_Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cc4062f",
      "metadata": {
        "id": "7cc4062f"
      },
      "source": [
        "# Week 06 - Feature Engineering\n",
        "\n",
        "Name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "* Colab - get notebook from gitmystuff DTSC4050 repository\n",
        "* Save a Copy in Drive\n",
        "* Remove Copy of\n",
        "* Edit name\n",
        "* Take attendance\n",
        "* Clean up Colab Notebooks folder\n",
        "* Submit shared link\n"
      ],
      "metadata": {
        "id": "bGezCn18xkLd"
      },
      "id": "bGezCn18xkLd"
    },
    {
      "cell_type": "markdown",
      "id": "0ea41731",
      "metadata": {
        "id": "0ea41731"
      },
      "source": [
        "## Correlation\n",
        "\n",
        "In statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data. Although in the broadest sense, \"correlation\" may indicate any type of association, in statistics it usually refers to the degree to which a pair of variables are linearly related. Familiar examples of dependent phenomena include the correlation between the height of parents and their offspring, and the correlation between the price of a good and the quantity the consumers are willing to purchase, as it is depicted in the so-called demand curve.\n",
        "\n",
        "Correlations are useful because they can indicate a predictive relationship that can be exploited in practice. For example, an electrical utility may produce less power on a mild day based on the correlation between electricity demand and weather. In this example, there is a causal relationship, because extreme weather causes people to use more electricity for heating or cooling. However, in general, the presence of a correlation is not sufficient to infer the presence of a causal relationship (i.e., correlation does not imply causation).\n",
        "\n",
        "https://en.wikipedia.org/wiki/Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f5a5a3",
      "metadata": {
        "id": "14f5a5a3"
      },
      "outputs": [],
      "source": [
        "# # data from https://www.kaggle.com/rohankayan/years-of-experience-and-salary-dataset\n",
        "# import pandas as pd\n",
        "\n",
        "# salary = pd.read_csv('https://raw.githubusercontent.com/gitmystuff/Datasets/main/Salary_Data.csv')\n",
        "# salary.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2301fe9",
      "metadata": {
        "id": "d2301fe9"
      },
      "outputs": [],
      "source": [
        "# # bivariate scatter plot\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# sns.regplot(data=salary, x='YearsExperience', y='Salary', ci=False)\n",
        "# plt.title('Correlation with Line of Best Fit');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89ed403e",
      "metadata": {
        "id": "89ed403e"
      },
      "source": [
        "### Spurious Correlations\n",
        "\n",
        "https://www.tylervigen.com/spurious-correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf554c0d",
      "metadata": {
        "id": "cf554c0d"
      },
      "source": [
        "### Pearson's Correlation Coefficient\n",
        "\n",
        "A measure of linear correlation between two sets of data. It is the ratio between the covariance of two variables and the product of their standard deviations; thus, it is essentially a normalized measurement of the covariance.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n",
        "\n",
        "Galton conceived the idea of correlation as a measure of how two variables relate such as heights and arm lengths\n",
        "\n",
        "$r = \\rho_{x,y} = \\frac{cov(x,y)}{\\sigma_x\\sigma_y} = \\frac{\\frac{1}{N}\\sum(x-\\bar{x})(y-\\bar{y})}{\\sqrt\\frac{\\sum(x-\\bar{x})^2}{N}\\sqrt\\frac{\\sum(y-\\bar{y})^2}{N}}  = \\frac{\\sum(x-\\bar{x})(y-\\bar{y})}{\\sqrt{\\sum(x-\\bar{x})^2}\\sqrt{\\sum(y-\\bar{y})^2}}$\n",
        "\n",
        "* r has a range of -1 to 1\n",
        "* When it equals (-)1, one feature can predict another feature precisely\n",
        "* When it equals 0, the prediction is no better than a random guess\n",
        "* The slope is agnostic in regards to cause and effect\n",
        "* One could cause the other, or there may be a third variable controlling both."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c722e0ff",
      "metadata": {
        "id": "c722e0ff"
      },
      "source": [
        "$\\frac{cov(x,y)}{\\sigma_x\\sigma_y} = \\frac{\\frac{1}{N}\\sum(x-\\bar{x})(y-\\bar{y})}{\\sqrt\\frac{\\sum(x-\\bar{x})^2}{N}\\sqrt\\frac{\\sum(y-\\bar{y})^2}{N}}$\n",
        "\n",
        "* Covariance measures the directional relationship between the returns on two assets. A positive covariance means asset returns move together, while a negative covariance means they move inversely. Covariance is calculated by analyzing at-return surprises (standard deviations from the expected return) or multiplying the correlation between the two random variables by the standard deviation of each variable. https://www.investopedia.com/terms/c/covariance.asp\n",
        "* A standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean. Low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out. https://www.nlm.nih.gov/nichsr/stats_tutorial/section2/mod8_sd.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce37da4b",
      "metadata": {
        "id": "ce37da4b"
      },
      "source": [
        "**r = 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c13bfb",
      "metadata": {
        "id": "54c13bfb"
      },
      "outputs": [],
      "source": [
        "# import sklearn\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "\n",
        "# y = pd.Series([1, 2, 3, 4, 5, 6])\n",
        "# x = pd.Series([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "# correlation = y.corr(x)\n",
        "# print('r = ', correlation)\n",
        "\n",
        "# # plotting the data\n",
        "# plt.scatter(x, y)\n",
        "\n",
        "# # This will fit the best line into the graph\n",
        "# plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))\n",
        "#          (np.unique(x)), color='red');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**r = -1**"
      ],
      "metadata": {
        "id": "0BXsQa850aGN"
      },
      "id": "0BXsQa850aGN"
    },
    {
      "cell_type": "code",
      "source": [
        "# import sklearn\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "\n",
        "# y = pd.Series([1, 2, 3, 4, 5, 6])\n",
        "# x = pd.Series([6, 5, 4, 3, 2, 1])\n",
        "\n",
        "# correlation = y.corr(x)\n",
        "# print('r = ', correlation)\n",
        "\n",
        "# # plotting the data\n",
        "# plt.scatter(x, y)\n",
        "\n",
        "# # This will fit the best line into the graph\n",
        "# plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))\n",
        "#          (np.unique(x)), color='red');"
      ],
      "metadata": {
        "id": "_E4kvSRp0dbk"
      },
      "id": "_E4kvSRp0dbk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5b6bbd34",
      "metadata": {
        "id": "5b6bbd34"
      },
      "source": [
        "**r = 0**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50dcbf8d",
      "metadata": {
        "id": "50dcbf8d"
      },
      "outputs": [],
      "source": [
        "# import sklearn\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "\n",
        "# y = pd.Series([9, 4, 1, 0, 1, 4, 9])\n",
        "# x = pd.Series([-3, -2, -1, 0, 1, 2, 3])\n",
        "\n",
        "# correlation = y.corr(x)\n",
        "# print('r = ', correlation)\n",
        "\n",
        "# plt.scatter(x, y)\n",
        "\n",
        "# # line of best fit\n",
        "# plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))\n",
        "#          (np.unique(x)), color='red');"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# salary.corr()"
      ],
      "metadata": {
        "id": "Lfbcr4hP0rbf"
      },
      "id": "Lfbcr4hP0rbf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spearman's Rank Correlation\n",
        "\n",
        "$\\rho = 1 - \\frac{6\\sum{d_i^2}}{n(n^2-1)}$\n",
        "\n",
        "* $\\rho$ = Spearman's rank correlation coefficient\n",
        "* $d^i$ = difference between the two ranks of each observation\n",
        "* $n$ = number of observations\n",
        "\n",
        "Definition\n",
        "\n",
        "* A Spearman correlation coefficient is also referred to as Spearman rank correlation or Spearman’s rho.  It is typically denoted either with the Greek letter rho (ρ), or rs.  Like all correlation coefficients, Spearman’s rho measures the strength of association between two variables.  As such, the Spearman correlation coefficient is similar to the Pearson correlation coefficient.\n",
        "\n",
        "Sources\n",
        "* https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/spearman-rank-correlation/\n",
        "* https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient"
      ],
      "metadata": {
        "id": "xO-v4po5zJEx"
      },
      "id": "xO-v4po5zJEx"
    },
    {
      "cell_type": "code",
      "source": [
        "# # data from https://data.mendeley.com/datasets/tttrffnjtd/1\n",
        "# import pandas as pd\n",
        "\n",
        "# spearman = pd.read_excel('https://raw.githubusercontent.com/gitmystuff/Datasets/main/Spearman.xls', index_col=0)\n",
        "# spearman.head()"
      ],
      "metadata": {
        "id": "lPkKC_1Z5yhU"
      },
      "id": "lPkKC_1Z5yhU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # method = spearman\n",
        "# spearman._get_numeric_data().corr(method='spearman')"
      ],
      "metadata": {
        "id": "azKYnRrF_Cbx"
      },
      "id": "azKYnRrF_Cbx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "715565d5",
      "metadata": {
        "id": "715565d5"
      },
      "source": [
        "## Some History\n",
        "\n",
        "Content for the following individuals draw heavily on Wikipedia and The Book of Why Chapter 2 by Judea Pearl and Dana Mackenzie\n",
        "\n",
        "### Pearson\n",
        "\n",
        "* That's Karl Pearson with a C\n",
        "* Pearson felt that Galton did away with causation and 1 was just perfect correlation\n",
        "* Data is all there is to science\n",
        "* Galton says that relationships didn't need a casual explanation\n",
        "* Pearson went further by removing causation from science\n",
        "* Pearson belonged to the Positivist School which holds that the universe is a product of human thought and that science is just the expression of these thoughts\n",
        "* Thus causation, outside our thoughts, does not exist\n",
        "* Thoughts can only reflect patterns of observations and can be completely described by correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "922d6fd0",
      "metadata": {
        "id": "922d6fd0"
      },
      "source": [
        "### Pearson's Background\n",
        "\n",
        "* 1857 - 1936\n",
        "* An English mathematician and biostatistician\n",
        "* Spent most of the 1880s in Germany / Austria\n",
        "* Loved Germany so much he changed his name from Carl to Karl\n",
        "* A women's right and equality activist, founder of the Men and Women's Club\n",
        "* A socialist that offered help translating some of Karl Marx's work (Das Kapital)\n",
        "* Secured a grant for a biometrics lab at the University of College London\n",
        "* The lab became a department when Galton passed and left an endowment for a professorship as long as Pearson was the first holder\n",
        "* Had to explain what he calls genuine (organic) correlation and spurious correlation\n",
        "* For example, there's a strong correlation between a country's chocolate consumption and Nobel Prize winners"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d32f6f0a",
      "metadata": {
        "id": "d32f6f0a"
      },
      "source": [
        "### Sewall Wright and Guinea Pigs\n",
        "\n",
        "* 1889 - 1988\n",
        "* Wright went to Harvard to study genetics and about 1915 got a job with the USDA taking care of Guinea Pigs\n",
        "* The Guinea Pigs turned out to be the spring board to Wright's success\n",
        "* Evolution was not gradual, as Darwin posited, but happens in bursts\n",
        "* 1925, Wright was faculty at University of Chicago and stayed close to Guinea Pigs\n",
        "* A story is that he was handling a Gunea Pig while lecturing at the chalk board and mistanely used the Guinea Pig to erase the board\n",
        "* Guinea Pig coat color refused to play by the genetic understanding of the time\n",
        "* It proved impossible to breed an all white / all colored guinea pig\n",
        "* Even the most inbred had a wide variation\n",
        "* Wright postulated that genetics alone governed coat color and added developmental factors in the womb\n",
        "* Something in the womb was `causing` coat color\n",
        "* 20 generations eliminated the genetic variation while maintaining the developmental factors\n",
        "* Wright, even though right, was severely attacked at the time by Pearson's disciples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0e4eaf",
      "metadata": {
        "id": "dd0e4eaf"
      },
      "source": [
        "### R. A. Fisher\n",
        "\n",
        "* 1890 - 1962\n",
        "* Popularized the p-value\n",
        "* Linear discriminant analysis\n",
        "* F-distribution\n",
        "* Student's t-distribution\n",
        "* He was from an early age a supporter of certain eugenic ideas, and it is for this reason that he has been accused of being a racist and an advocate of forced sterilisation (Evans 2020). His promotion of eugenics has recently caused various organisations to remove his name from awards and dedications of buildings (Tarran 2020; Rothamsted Research 2020; Society for the Study of Evolution 2020; Gonville and Caius College 2020). https://www.nature.com/articles/s41437-020-00394-6\n",
        "* Rival with Wright\n",
        "* Worked as a statistician in the City of London and taught physics and maths\n",
        "* Statistics my be regarded as the study of methods of the reductions of data\n",
        "* Wright argued that statistics was more than just a collection of mathematical methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8614e8b8",
      "metadata": {
        "id": "8614e8b8"
      },
      "source": [
        "## Example of Engineering a Feature by Transforming its Values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dbb75c0",
      "metadata": {
        "id": "5dbb75c0"
      },
      "source": [
        "### Logarithm and Moore's Law\n",
        "\n",
        "Moore's law is the observation that the number of transistors in a dense integrated circuit (IC) doubles about every two years. Moore's law is an observation and projection of a historical trend. Rather than a law of physics, it is an empirical relationship linked to gains from experience in production.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Moore's_law"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4745aa5",
      "metadata": {
        "id": "d4745aa5"
      },
      "outputs": [],
      "source": [
        "# # get the data\n",
        "# import pandas as pd\n",
        "\n",
        "# moores = pd.read_csv('https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv', header=None)\n",
        "# moores.columns = ['year', 'transistors']\n",
        "# moores.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0897ddb2",
      "metadata": {
        "id": "0897ddb2"
      },
      "outputs": [],
      "source": [
        "# # plot the data\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.scatter(moores['year'], moores['transistors']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ddd9ee8",
      "metadata": {
        "id": "2ddd9ee8"
      },
      "outputs": [],
      "source": [
        "# # apply log to transistors\n",
        "# import numpy as np\n",
        "\n",
        "# moores['log_trans'] = np.log(moores.transistors)\n",
        "# plt.scatter(moores['year'], moores['log_trans']);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6416fcd5",
      "metadata": {
        "id": "6416fcd5"
      },
      "source": [
        "### More on Logarithms\n",
        "\n",
        "* 10 * 10 (10 and 100)\n",
        "* 10 * 10 * 10 (10 and 1000)\n",
        "* power of 0 = 1 (single item)\n",
        "* power of 1 = 10\n",
        "* power of 3 = thousand\n",
        "* power of 6 = million\n",
        "* power of 9 = billion\n",
        "* power of 12 = trillion\n",
        "* power of 23 = number of molecules in a dozen grams of carbon\n",
        "* power of 80 = number of molecules in the universe\n",
        "\n",
        "A 0 to 80 scale took us from a single item to the number of things in the universe.\n",
        "\n",
        "https://betterexplained.com/articles/using-logs-in-the-real-world/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37f760b2",
      "metadata": {
        "id": "37f760b2"
      },
      "source": [
        "### Multicollinearity\n",
        "\n",
        "* Makes it difficult to determine which independent variables are influencing the dependent variable"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef0eb76d",
      "metadata": {
        "id": "ef0eb76d"
      },
      "source": [
        "### Correlation vs Multicollinearity\n",
        "\n",
        "* Correlation measures how two or more variables move together (good between independent and dependent variables)\n",
        "* (Mutli)collinearity shows a linear relationship, usually high, between features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54b8f0c9",
      "metadata": {
        "id": "54b8f0c9"
      },
      "source": [
        "### Correlation Between Features\n",
        "\n",
        "* anything above .9 do something about it\n",
        "* between .5 and .7 may need a closer look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2343619b",
      "metadata": {
        "id": "2343619b"
      },
      "outputs": [],
      "source": [
        "# # data from https://www.kaggle.com/rohankayan/years-of-experience-and-salary-dataset\n",
        "# import pandas as pd\n",
        "\n",
        "# salary = pd.read_csv('https://raw.githubusercontent.com/gitmystuff/INFO4050/main/Datasets/Salary_Data.csv')\n",
        "# salary.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96595f1d",
      "metadata": {
        "id": "96595f1d"
      },
      "outputs": [],
      "source": [
        "# X_salary = salary.drop('Salary', axis=1)\n",
        "# y_salary = salary['Salary']\n",
        "# X_salary.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4950d6be",
      "metadata": {
        "id": "4950d6be"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# # correlation matrix\n",
        "# sns.set(style=\"white\")\n",
        "\n",
        "# # compute the correlation matrix\n",
        "# corr = X_salary.corr()\n",
        "\n",
        "# # generate a mask for the upper triangle\n",
        "# mask = np.zeros_like(corr, dtype=bool)\n",
        "# mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "# # set up the matplotlib figure\n",
        "# f, ax = plt.subplots()\n",
        "\n",
        "# # generate a custom diverging colormap\n",
        "# cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# # draw the heatmap with the mask and correct aspect ratio\n",
        "# sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "#             square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af6c1d1c",
      "metadata": {
        "id": "af6c1d1c"
      },
      "source": [
        "### Derived Variables\n",
        "\n",
        "age and salary usually are correlated but ambition can create outliers because a younger person can make a million off a great idea or an older person may be an artist etc.\n",
        "\n",
        "Ambition = YearsExperience / Age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1b87d9c",
      "metadata": {
        "id": "d1b87d9c"
      },
      "outputs": [],
      "source": [
        "# # combine YearsExperience and Age\n",
        "# X_salary['Ambition'] = X_salary['YearsExperience'] / X_salary['Age']\n",
        "# salary['Ambition'] = salary['YearsExperience'] / salary['Age']\n",
        "# X_salary.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86a7bb7f",
      "metadata": {
        "id": "86a7bb7f"
      },
      "outputs": [],
      "source": [
        "# # correlation matrix\n",
        "# sns.set(style=\"white\")\n",
        "\n",
        "# # compute the correlation matrix\n",
        "# corr = X_salary.corr()\n",
        "\n",
        "# # generate a mask for the upper triangle\n",
        "# mask = np.zeros_like(corr, dtype=bool)\n",
        "# mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "# # set up the matplotlib figure\n",
        "# f, ax = plt.subplots()\n",
        "\n",
        "# # generate a custom diverging colormap\n",
        "# cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# # draw the heatmap with the mask and correct aspect ratio\n",
        "# sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "#             square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b9244ec",
      "metadata": {
        "id": "6b9244ec"
      },
      "outputs": [],
      "source": [
        "# # showing correlation of multiple features with one target\n",
        "# X_salary.corrwith(y_salary).plot.bar(\n",
        "#         title = \"Correlation with Target\", fontsize = 15,\n",
        "#         rot = 45, grid = True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcf1a7d0",
      "metadata": {
        "id": "bcf1a7d0"
      },
      "outputs": [],
      "source": [
        "# # showing correlation of multiple features with one target\n",
        "# X_salary.drop(['YearsExperience', 'Age'], axis=1).corrwith(y_salary).plot.bar(\n",
        "#         title = \"Correlation with Target\", fontsize = 15,\n",
        "#         rot = 45, grid = True);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "415adab4",
      "metadata": {
        "id": "415adab4"
      },
      "source": [
        "### Mean, Median, Mode Imputation\n",
        "\n",
        "* Mean if normal\n",
        "* Median if skewed\n",
        "* Used for MCAR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # get data\n",
        "# import pandas as pd\n",
        "\n",
        "# houses = pd.read_csv('https://raw.githubusercontent.com/gitmystuff/Datasets/main/house-prices.csv')\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     houses.drop('SalePrice', axis=1),\n",
        "#     houses['SalePrice'],\n",
        "#     test_size=0.25,\n",
        "#     random_state=42)\n",
        "\n",
        "# X_train.head()"
      ],
      "metadata": {
        "id": "HL4APoTAABj0"
      },
      "id": "HL4APoTAABj0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # find nulls\n",
        "# X_train.info()"
      ],
      "metadata": {
        "id": "AesVK55RA0Kk"
      },
      "id": "AesVK55RA0Kk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train.isnull().sum()"
      ],
      "metadata": {
        "id": "Ois4HcLTBnuq"
      },
      "id": "Ois4HcLTBnuq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for feat in X_train.columns:\n",
        "#     if X_train[feat].isnull().any():\n",
        "#         print(feat, X_train[feat].isnull().sum())"
      ],
      "metadata": {
        "id": "zh_1vB8hBsl2"
      },
      "id": "zh_1vB8hBsl2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nulls = [feat for feat in X_train.columns if X_train[feat].isnull().any()]\n",
        "# nulls"
      ],
      "metadata": {
        "id": "WDAI8csVBvYP"
      },
      "id": "WDAI8csVBvYP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0568e5b9",
      "metadata": {
        "id": "0568e5b9"
      },
      "outputs": [],
      "source": [
        "# # example of some nulls\n",
        "# X_train[['LotFrontage', 'MasVnrArea', 'GarageYrBlt']].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4023bf40",
      "metadata": {
        "id": "4023bf40"
      },
      "outputs": [],
      "source": [
        "# X_train[['LotFrontage', 'MasVnrArea', 'GarageYrBlt']].hist();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb279d8a",
      "metadata": {
        "id": "cb279d8a"
      },
      "outputs": [],
      "source": [
        "# # fill na with mean median mode\n",
        "# mmm = pd.DataFrame(columns = ['LotFrontage', 'MasVnrArea', 'GarageYrBlt'])\n",
        "# mmm['LotFrontage'] = X_train['LotFrontage'].fillna(round(X_train['LotFrontage'].mean(), 2))\n",
        "# mmm['MasVnrArea'] = X_train['MasVnrArea'].fillna(X_train['MasVnrArea'].median())\n",
        "# mmm['GarageYrBlt'] = X_train['GarageYrBlt'].fillna(X_train['GarageYrBlt'].mode()[0])\n",
        "# mmm.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3537932",
      "metadata": {
        "id": "b3537932"
      },
      "source": [
        "### Arbitrary Constants\n",
        "\n",
        "* Discovers if MNAR\n",
        "* Goal is to flag missing values\n",
        "* Use values not in distribution\n",
        "* Importance of missingness if present\n",
        "* Depends on the model (Linear models maybe not because more arbitrary values in distribution, Trees maybe)\n",
        "\n",
        "We don't want to impute mean, median, etc because it looks like the data. We want to emphasize the missing data because we believe it's missing not at random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94fa86b1",
      "metadata": {
        "id": "94fa86b1"
      },
      "outputs": [],
      "source": [
        "# # recall missing values (non-null)\n",
        "# print(X_train.shape)\n",
        "# print(X_train[nulls].info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fbad662",
      "metadata": {
        "id": "3fbad662"
      },
      "outputs": [],
      "source": [
        "# X_train['GarageType'].fillna('Missing', inplace=True)\n",
        "# X_train['GarageType'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b41f7c7",
      "metadata": {
        "id": "6b41f7c7"
      },
      "source": [
        "### End of Distribution\n",
        "\n",
        "* If normal we can use -3, 3 standard deviations\n",
        "* If skewed we can use IQR proximity rule (iqr x 1.5, or iqr x 3)\n",
        "* Flag the missing value where observations are rarely represented\n",
        "* Used in finances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eaa8bc2",
      "metadata": {
        "id": "8eaa8bc2"
      },
      "outputs": [],
      "source": [
        "# # histogram of LotFrontage\n",
        "# X_train['LotFrontage'].hist();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b44dc1",
      "metadata": {
        "id": "17b44dc1"
      },
      "outputs": [],
      "source": [
        "# # boxplot of LotFrontage\n",
        "# X_train.boxplot('LotFrontage');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92cfd338",
      "metadata": {
        "id": "92cfd338"
      },
      "outputs": [],
      "source": [
        "# # iqr as na\n",
        "# iqr = X_train['LotFrontage'].quantile(0.75) - X_train['LotFrontage'].quantile(0.25)\n",
        "# end_of_distribution = X_train['LotFrontage'].quantile(0.75) + (1.5 * iqr)\n",
        "# X_train['LotFrontage_Imputed'] = X_train['LotFrontage'].fillna(end_of_distribution)\n",
        "# print(end_of_distribution)\n",
        "# print(X_train['LotFrontage_Imputed'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc3f8ec",
      "metadata": {
        "id": "0bc3f8ec"
      },
      "outputs": [],
      "source": [
        "# X_train.boxplot('LotFrontage_Imputed');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28c3f668",
      "metadata": {
        "id": "28c3f668"
      },
      "source": [
        "## Categorical Encoding\n",
        "* Sklearn One Hot Encoding\n",
        "* Dummy Trap\n",
        "* Pandas get_dummies\n",
        "* Labelizer\n",
        "* Weight of Evidence\n",
        "* Frequency Encoding\n",
        "\n",
        "### Categorical Data\n",
        "* Nominal (Cat or Dog)\n",
        "* Ordinal (Grades)\n",
        "* Works better for limited labels in a category\n",
        "* Engineer features with many labels\n",
        "\n",
        "### Multicollinearity\n",
        "* Predictors need to be independent of each other\n",
        "* https://www.theanalysisfactor.com/multicollinearity-explained-visually/\n",
        "* https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/\n",
        "* Cats_and_Dogs = [Cat, Dog, Dog, Cat, Cat, Dog]\n",
        "* Cats = [1, 0, 0, 1, 1, 0]\n",
        "* Dogs = [0, 1, 1, 0, 0, 1]\n",
        "\n",
        "### Mismatch in Training and Test\n",
        "\n",
        "* Some labels in the train set don't show up in the test set\n",
        "\n",
        "https://towardsdatascience.com/beware-of-the-dummy-variable-trap-in-pandas-727e8e6b8bde"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1beccfa2",
      "metadata": {
        "id": "1beccfa2"
      },
      "source": [
        "### One Hot Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e2451d6",
      "metadata": {
        "id": "7e2451d6"
      },
      "outputs": [],
      "source": [
        "# # sklearn OneHotEncoder\n",
        "# # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
        "# # https://stackoverflow.com/questions/50473381/scikit-learns-labelbinarizer-vs-onehotencoder\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# pets = ['dog', 'cat', 'cat', 'dog', 'turtle', 'cat', 'cat', 'turtle', 'dog', 'cat']\n",
        "# print('cat = 0; dog = 1; turtle = 2')\n",
        "# le = LabelEncoder()\n",
        "# int_values = le.fit_transform(pets)\n",
        "# print('Pets:', pets)\n",
        "# print('Label Encoder:', int_values)\n",
        "# int_values = int_values.reshape(len(int_values), 1)\n",
        "# print(pd.Series(pets))\n",
        "\n",
        "# ohe = OneHotEncoder(sparse_output=False)\n",
        "# ohe = ohe.fit_transform(int_values)\n",
        "# print('One Hot Encoder:\\n', ohe)\n",
        "\n",
        "# lb = LabelBinarizer()\n",
        "# print('Label Binarizer:\\n', lb.fit_transform(int_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a558af81",
      "metadata": {
        "id": "a558af81"
      },
      "outputs": [],
      "source": [
        "# pets = pd.DataFrame(pd.Series(pets), columns=['Pets'])\n",
        "# pets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3af38b01",
      "metadata": {
        "id": "3af38b01"
      },
      "outputs": [],
      "source": [
        "# ohe = OneHotEncoder(sparse_output=False)\n",
        "# ohe_pets = ohe.fit_transform(pets)\n",
        "# pets_df = pd.DataFrame(ohe_pets, columns=ohe.get_feature_names_out(['Pets']))\n",
        "# pets_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eaf9f4e",
      "metadata": {
        "id": "3eaf9f4e"
      },
      "source": [
        "### Dummy Trap\n",
        "\n",
        "The Dummy Variable Trap occurs when two or more dummy variables created by one-hot encoding are highly correlated (multi-collinear). This means that one variable can be predicted from the others, making it difficult to interpret predicted coefficient variables in regression models. In other words, the individual effect of the dummy variables on the prediction model can not be interpreted well because of multicollinearity.\n",
        "\n",
        "https://www.learndatasci.com/glossary/dummy-variable-trap/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f408874",
      "metadata": {
        "id": "7f408874"
      },
      "outputs": [],
      "source": [
        "# pets_df.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ec8b974",
      "metadata": {
        "id": "5ec8b974"
      },
      "outputs": [],
      "source": [
        "# ohe = OneHotEncoder(drop='first', sparse_output=False)\n",
        "# ohe_pets = ohe.fit_transform(pets)\n",
        "# pets_df = pd.DataFrame(ohe_pets, columns=ohe.get_feature_names_out(['Pets']))\n",
        "# pets_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2271c79c",
      "metadata": {
        "id": "2271c79c"
      },
      "outputs": [],
      "source": [
        "# pets_df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Day of Week Encoding\n",
        "\n",
        "* https://mikulskibartosz.name/time-in-machine-learning"
      ],
      "metadata": {
        "id": "6yia75mH8p43"
      },
      "id": "6yia75mH8p43"
    },
    {
      "cell_type": "markdown",
      "id": "084c15ac",
      "metadata": {
        "id": "084c15ac"
      },
      "source": [
        "### Get Dummies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "576cb600",
      "metadata": {
        "id": "576cb600"
      },
      "outputs": [],
      "source": [
        "# # using pandas get_dummies\n",
        "# import pandas as pd\n",
        "\n",
        "# X_dummy = pd.get_dummies(X_train[['GarageType', 'GarageQual']], drop_first=True)\n",
        "# y_dummy = pd.get_dummies(X_test[['GarageType', 'GarageQual']], drop_first=True)\n",
        "# print(X_dummy.shape)\n",
        "# print(y_dummy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "270bc2d6",
      "metadata": {
        "id": "270bc2d6"
      },
      "outputs": [],
      "source": [
        "# # using one hot encoder\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# ohe = OneHotEncoder(drop='first', sparse_output=False)\n",
        "\n",
        "# ohe_train = ohe.fit_transform(X_train[['GarageType', 'GarageQual']].dropna())\n",
        "# ohe_train = pd.DataFrame(ohe_train, columns=ohe.get_feature_names_out(['GarageType', 'GarageQual']))\n",
        "# print(ohe_train.shape)\n",
        "# ohe_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2c740cf",
      "metadata": {
        "id": "e2c740cf"
      },
      "outputs": [],
      "source": [
        "# # ohe is already trained\n",
        "# ohe_test = ohe.transform(X_test[['GarageType', 'GarageQual']].dropna())\n",
        "# ohe_test = pd.DataFrame(ohe_test, columns=ohe_train.columns)\n",
        "# print(ohe_test.shape)\n",
        "# ohe_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45820551",
      "metadata": {
        "id": "45820551"
      },
      "source": [
        "### One Hot Encoding Alternatives\n",
        "\n",
        "For features with many labels\n",
        "\n",
        "* https://medium.com/analytics-vidhya/stop-one-hot-encoding-your-categorical-variables-bbb0fba89809\n",
        "* https://medium.com/swlh/stop-one-hot-encoding-your-categorical-features-avoid-curse-of-dimensionality-16743c32cea4\n",
        "* https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02 (frequency and mean encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b079a6a9",
      "metadata": {
        "id": "b079a6a9"
      },
      "outputs": [],
      "source": [
        "# # review features with multiple labels\n",
        "# # identify features with more than 5 features\n",
        "# mult_labels = []\n",
        "# freq_feats = []\n",
        "\n",
        "# for val in X_train.columns.sort_values():\n",
        "#   if val in nulls:\n",
        "#     print(val, len(X_train[val].dropna().unique()))\n",
        "#     mult_labels.append(val)\n",
        "#     if len(X_train[val].dropna().unique()) > 4:\n",
        "#       freq_feats.append(val)\n",
        "\n",
        "# print(mult_labels)\n",
        "# print(freq_feats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e742289",
      "metadata": {
        "id": "1e742289"
      },
      "outputs": [],
      "source": [
        "# # fill frequency\n",
        "# print(X_train['GarageType'].value_counts())\n",
        "# for feat in freq_feats:\n",
        "#     freq = X_train.groupby(feat).size()/len(X_train)\n",
        "#     X_train[feat] = X_train[feat].map(freq)\n",
        "#     freq = X_test.groupby(feat).size()/len(X_test)\n",
        "#     X_test[feat] = X_test[feat].map(freq)\n",
        "\n",
        "# print(X_train['GarageType'].value_counts())\n",
        "# print(X_train['GarageType'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e5049aa",
      "metadata": {
        "id": "5e5049aa"
      },
      "source": [
        "### Bi-Label Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dba2646",
      "metadata": {
        "id": "6dba2646"
      },
      "outputs": [],
      "source": [
        "# # get and train test split data\n",
        "# import seaborn as sns\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# titanic = sns.load_dataset('titanic')\n",
        "# X_train, X_test, y_train, y_test = train_test_split(titanic.drop(['survived'], axis=1), titanic['survived'], test_size=.25, random_state=42)\n",
        "# X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a98ba2",
      "metadata": {
        "id": "48a98ba2"
      },
      "outputs": [],
      "source": [
        "# titanic.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af697a72",
      "metadata": {
        "id": "af697a72"
      },
      "outputs": [],
      "source": [
        "# # bi-label mapping\n",
        "# # whatever you do for X_train, do for X_test\n",
        "# X_train['sex'] = X_train['sex'].map({'male':0,'female':1})\n",
        "# X_test['sex'] = X_test['sex'].map({'male':0,'female':1})\n",
        "# X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38db741f",
      "metadata": {
        "id": "38db741f"
      },
      "source": [
        "### Encoding Order\n",
        "\n",
        "* Bilabel Mapping (2 labels)\n",
        "* Frequency (5+ labels)\n",
        "* One Hot Encoding (3 - 5 labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8598a4b1",
      "metadata": {
        "id": "8598a4b1"
      },
      "source": [
        "## Outliers\n",
        "\n",
        "* Treat outliers as missing data and impute accordingly\n",
        "* Impose min max values\n",
        "* Take care of altering meaningful data\n",
        "* Outliers should be detected and removed from train only\n",
        "\n",
        "https://www.projectpro.io/recipes/deal-with-outliers-in-python\n",
        "\n",
        "* Drop\n",
        "* Mark\n",
        "* Rescale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "940596a5",
      "metadata": {
        "id": "940596a5"
      },
      "outputs": [],
      "source": [
        "# # get data\n",
        "# from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# housing = fetch_california_housing()\n",
        "# print(housing.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d33784",
      "metadata": {
        "id": "b6d33784"
      },
      "outputs": [],
      "source": [
        "# # get keys\n",
        "# housing.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dda64127",
      "metadata": {
        "id": "dda64127"
      },
      "outputs": [],
      "source": [
        "# # create housing dataframe\n",
        "# import pandas as pd\n",
        "\n",
        "# housing_df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "# housing_df['MedHouseVal'] = housing.target\n",
        "# housing_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43dd0266",
      "metadata": {
        "id": "43dd0266"
      },
      "outputs": [],
      "source": [
        "# # train test split\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     housing_df.drop('MedHouseVal', axis=1),\n",
        "#     housing_df['MedHouseVal'],\n",
        "#     test_size=0.25,\n",
        "#     random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cea99d3d",
      "metadata": {
        "id": "cea99d3d"
      },
      "outputs": [],
      "source": [
        "# # histograms\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# X_train.hist()\n",
        "# plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32083b0e",
      "metadata": {
        "id": "32083b0e"
      },
      "outputs": [],
      "source": [
        "# # boxplots\n",
        "# X_train.boxplot(rot=45)\n",
        "# plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46f68766",
      "metadata": {
        "id": "46f68766"
      },
      "outputs": [],
      "source": [
        "# X_train.boxplot('MedInc');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c412c887",
      "metadata": {
        "id": "c412c887"
      },
      "outputs": [],
      "source": [
        "# import seaborn as sns\n",
        "\n",
        "# sns.violinplot(x=X_train['MedInc']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb046d6e",
      "metadata": {
        "id": "eb046d6e"
      },
      "outputs": [],
      "source": [
        "# # prob plot\n",
        "# import scipy.stats as stats\n",
        "\n",
        "# stats.probplot(X_train['MedInc'], plot=plt);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d10a61b2",
      "metadata": {
        "id": "d10a61b2"
      },
      "source": [
        "#### Boxplot and Normal Curve Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d745e2f",
      "metadata": {
        "id": "3d745e2f"
      },
      "outputs": [],
      "source": [
        "# # compare boxplot with normal distribution\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import scipy.stats as stats\n",
        "\n",
        "# data = stats.norm.rvs(size=1000)\n",
        "# mu, std = stats.norm.fit(data)\n",
        "\n",
        "# x = np.linspace(-3, 3, 1000)\n",
        "# p = stats.norm.pdf(x, mu, std)\n",
        "# plt.plot(x, p, 'k', linewidth=2)\n",
        "# plt.boxplot(data, vert=False)\n",
        "# plt.xlabel('Values')\n",
        "# plt.ylabel('Probabilities')\n",
        "# plt.title(f'mu = {mu:.2f},  std = {std:.2f}')\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d3bb198",
      "metadata": {
        "id": "8d3bb198"
      },
      "outputs": [],
      "source": [
        "# # compare with AveBedrms\n",
        "# X_train['MedInc'].plot.kde()\n",
        "# X_train.boxplot('MedInc', vert=False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b133ce96",
      "metadata": {
        "scrolled": true,
        "id": "b133ce96"
      },
      "outputs": [],
      "source": [
        "# # find iqr and inner outer boundaries\n",
        "# q1 = X_train['MedInc'].quantile(0.25)\n",
        "# q3 = X_train['MedInc'].quantile(0.75)\n",
        "# iqr = q3 - q1\n",
        "\n",
        "# lower_inner_fence = q1 - (1.5 * iqr)\n",
        "# upper_inner_fence = q3 + (1.5 * iqr)\n",
        "# lower_outer_fence = q1 - (1.5 * iqr)\n",
        "# upper_outer_fence = q3 + (1.5 * iqr)\n",
        "\n",
        "# print(f'Q1: {q1:.2f} - Q3: {q3:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19d61574",
      "metadata": {
        "id": "19d61574"
      },
      "outputs": [],
      "source": [
        "# # print outliers by feature\n",
        "# for feat in X_train._get_numeric_data().columns[1:]:\n",
        "#     q1 = X_train[feat].quantile(0.25)\n",
        "#     q3 = X_train[feat].quantile(0.75)\n",
        "#     iqr = q3 - q1\n",
        "#     lower_fence = (q1 - 1.5 * iqr)\n",
        "#     upper_fence = (q3 + 1.5 * iqr)\n",
        "#     lower_count = X_train[feat][X_train[feat] < lower_fence].count()\n",
        "#     upper_count = X_train[feat][X_train[feat] > upper_fence].count()\n",
        "#     print(f'{feat} outliers = {lower_count + upper_count}: lower_fence: {lower_fence}, upper_fence: {upper_fence}, lower_count: {lower_count}, upper_count: {upper_count}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df780c12",
      "metadata": {
        "id": "df780c12"
      },
      "outputs": [],
      "source": [
        "# # check our numbers\n",
        "# X_train['MedInc'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c392138",
      "metadata": {
        "id": "3c392138"
      },
      "source": [
        "### Outlier Trimming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fc2800b",
      "metadata": {
        "id": "5fc2800b"
      },
      "outputs": [],
      "source": [
        "# # flag the rows with outliers\n",
        "# import numpy as np\n",
        "\n",
        "# outliers = np.where(X_train['MedInc'] < lower_inner_fence, True,\n",
        "#                    np.where(X_train['MedInc'] > upper_inner_fence, True, False))\n",
        "\n",
        "# X_train_trimmed = X_train.loc[outliers]\n",
        "# print(X_train.shape, X_train_trimmed.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fcbcdbc",
      "metadata": {
        "id": "9fcbcdbc"
      },
      "source": [
        "### IQR Proximity Rule Capping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7d39473",
      "metadata": {
        "scrolled": true,
        "id": "d7d39473"
      },
      "outputs": [],
      "source": [
        "# # cap outliers\n",
        "# import scipy.stats as stats\n",
        "\n",
        "# X_train['capped'] = np.where(X_train['MedInc'] < lower_inner_fence, lower_inner_fence,\n",
        "#                    np.where(X_train['MedInc'] > upper_inner_fence, upper_inner_fence, X_train['MedInc']))\n",
        "\n",
        "# stats.probplot(X_train['capped'], plot=plt);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "889970a8",
      "metadata": {
        "id": "889970a8"
      },
      "source": [
        "## Scaling\n",
        "\n",
        "* Coefficients of linear models are influenced by the scale of the feature\n",
        "* Features with larger scales dominate smaller scales\n",
        "* Some algorithms, like PCA, require features to be centered at 0\n",
        "\n",
        "https://www.atoti.io/articles/when-to-perform-a-feature-scaling/\n",
        "\n",
        "* from sklearn.preprocessing import MinMaxScaler\n",
        "* from sklearn.preprocessing import minmax_scale\n",
        "* from sklearn.preprocessing import MaxAbsScaler\n",
        "* from sklearn.preprocessing import StandardScaler\n",
        "* from sklearn.preprocessing import RobustScaler\n",
        "* from sklearn.preprocessing import Normalizer\n",
        "* from sklearn.preprocessing import QuantileTransformer\n",
        "* from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98764eee",
      "metadata": {
        "id": "98764eee"
      },
      "source": [
        "### Standardization\n",
        "\n",
        "$z = \\frac{(x - \\bar{x})}{\\sigma}$\n",
        "\n",
        "* Centers data around 0\n",
        "* Scales the std to 1\n",
        "* Preserves original shape\n",
        "* Preserves outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86aa2e23",
      "metadata": {
        "id": "86aa2e23"
      },
      "outputs": [],
      "source": [
        "# X_train.drop('capped', axis=1, inplace=True)\n",
        "# X_train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "151db938",
      "metadata": {
        "id": "151db938"
      },
      "source": [
        "Characteristics of X_train\n",
        "* Mean values not centered around 0\n",
        "* Std not 1\n",
        "* Features have various magnitudes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb47dd54",
      "metadata": {
        "id": "cb47dd54"
      },
      "outputs": [],
      "source": [
        "# # standardize features\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit(X_train)\n",
        "# standardized_X = scaler.transform(X_train)\n",
        "# standardized_yX = scaler.transform(X_test) # we use the scaler that was trained on the X_train\n",
        "# X_train_standardized = pd.DataFrame(standardized_X, columns=X_train.columns)\n",
        "# X_train_standardized.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35b4037",
      "metadata": {
        "id": "f35b4037"
      },
      "outputs": [],
      "source": [
        "# # compare histograms\n",
        "# X_train.hist()\n",
        "# X_train_standardized.hist()\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dd52719",
      "metadata": {
        "id": "9dd52719"
      },
      "source": [
        "### MinMaxScaling (Normalization)\n",
        "\n",
        "* Does not center the mean around 0\n",
        "* Std (variance) differ\n",
        "* May not preserve original shape\n",
        "* 0 to 1 range\n",
        "* Sensitive to outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4669626",
      "metadata": {
        "id": "f4669626"
      },
      "outputs": [],
      "source": [
        "# # minmax scaling\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# scaler = MinMaxScaler()\n",
        "# scaler.fit(X_train)\n",
        "# minmax = scaler.transform(X_train)\n",
        "# # don't forget X_test\n",
        "# X_train_minmax = pd.DataFrame(minmax, columns=X_train.columns)\n",
        "# X_train_minmax.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d2e620d",
      "metadata": {
        "id": "1d2e620d"
      },
      "outputs": [],
      "source": [
        "# # visual comparison\n",
        "# X_train.hist()\n",
        "# X_train_minmax.hist()\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "103b867c",
      "metadata": {
        "id": "103b867c"
      },
      "source": [
        "### Mean Normalization\n",
        "\n",
        "* Centers the mean at 0\n",
        "* Std (variance) will differ\n",
        "* May alter original distribution\n",
        "* -1 to 1 range\n",
        "* Preserves outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4e8ad1e",
      "metadata": {
        "id": "d4e8ad1e"
      },
      "outputs": [],
      "source": [
        "# # find the means\n",
        "# means = X_train.mean(axis=0)\n",
        "# means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "030fe657",
      "metadata": {
        "id": "030fe657"
      },
      "outputs": [],
      "source": [
        "# # find the ranges\n",
        "# ranges = X_train.max(axis=0) - X_train.min(axis=0)\n",
        "# ranges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c8b79c1",
      "metadata": {
        "id": "9c8b79c1"
      },
      "outputs": [],
      "source": [
        "# # mean scale the data\n",
        "# X_train_meanscale = (X_train - means) / ranges\n",
        "# # don't forget X_test\n",
        "# X_train_meanscale.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6627095",
      "metadata": {
        "id": "a6627095"
      },
      "outputs": [],
      "source": [
        "# # visual comparison\n",
        "# X_train.hist()\n",
        "# X_train_meanscale.hist()\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab381088",
      "metadata": {
        "id": "ab381088"
      },
      "source": [
        "### RobustScaler\n",
        "\n",
        "* Replaces median with iqr\n",
        "* Variance varies\n",
        "* May not preserve distribution\n",
        "* Min max varies\n",
        "* Robust to outliers https://www.statisticshowto.com/robust-statistics/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af432858",
      "metadata": {
        "id": "af432858"
      },
      "outputs": [],
      "source": [
        "# # robust scaler\n",
        "# from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# scaler = RobustScaler()\n",
        "# scaler.fit(X_train)\n",
        "# robust = scaler.transform(X_train)\n",
        "# # don't forget X_test\n",
        "# X_train_robust = pd.DataFrame(robust, columns=X_train.columns)\n",
        "# X_train_robust.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "708f0ac5",
      "metadata": {
        "id": "708f0ac5"
      },
      "outputs": [],
      "source": [
        "# # visual comparison\n",
        "# X_train.hist()\n",
        "# X_train_robust.hist()\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7b6765f",
      "metadata": {
        "id": "e7b6765f"
      },
      "source": [
        "### PowerTransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80eefceb",
      "metadata": {
        "id": "80eefceb"
      },
      "outputs": [],
      "source": [
        "# # PowerTransformer scaler for outliers\n",
        "# from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# feat_scales = []\n",
        "\n",
        "# scaler = PowerTransformer()\n",
        "\n",
        "# for feat in feat_scales:\n",
        "#     X_train[feat] = scaler.fit_transform(X_train[feat].values.reshape(-1,1))\n",
        "\n",
        "# for feat in feat_scales:\n",
        "#     X_test[feat] = scaler.fit_transform(X_test[feat].values.reshape(-1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9945ceeb",
      "metadata": {
        "id": "9945ceeb"
      },
      "source": [
        "### Scaling to Unit Length\n",
        "\n",
        "* Scales a feature vector to 1, norm of 1\n",
        "* Normalizes feature not observation\n",
        "* Divides each observation vector by some norm\n",
        "* Manhattan distance (l1)\n",
        "* Euclidean distance (12)\n",
        "\n",
        "l1(X) = |x1| + |x2| ... + |xn|\n",
        "\n",
        "l2(X) = square root of x1^2 + x2^2 ... + xn^2\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Taxicab_geometry\n",
        "* https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\n",
        "* https://montjoile.medium.com/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c\n",
        "* https://www.atoti.io/articles/when-to-perform-a-feature-scaling/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d086598",
      "metadata": {
        "scrolled": true,
        "id": "8d086598"
      },
      "outputs": [],
      "source": [
        "# # unit length scaling\n",
        "# from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# scaler = Normalizer('l1')\n",
        "# scaler.fit(X_train)\n",
        "# unitlength = scaler.transform(X_train)\n",
        "# # don't forget X_test\n",
        "# X_train_unitlength = pd.DataFrame(unitlength, columns=X_train.columns)\n",
        "# X_train_unitlength.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d8e73f1",
      "metadata": {
        "id": "3d8e73f1"
      },
      "outputs": [],
      "source": [
        "# # recall values from original X_train\n",
        "# X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d01d75",
      "metadata": {
        "id": "88d01d75"
      },
      "outputs": [],
      "source": [
        "# # normalize the values\n",
        "# np.round( np.linalg.norm(X_train, ord=1, axis=1), 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afac943a",
      "metadata": {
        "id": "afac943a"
      },
      "outputs": [],
      "source": [
        "# # compare the following with the first value in X_train_unitlength.head() below\n",
        "# 4.2 / 1062"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3abd9a2",
      "metadata": {
        "id": "a3abd9a2"
      },
      "outputs": [],
      "source": [
        "# # see above\n",
        "# X_train_unitlength.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41fddb10",
      "metadata": {
        "id": "41fddb10"
      },
      "outputs": [],
      "source": [
        "# # visual comparison\n",
        "# X_train.hist()\n",
        "# X_train_unitlength.hist()\n",
        "# plt.tight_layout();"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}